<html>
  <head>
    <link rel="stylesheet" href="./style.css" />
  </head>

  <body>
    <h1>Project 3-1: Pathtracer ðŸ•º</h1>
    by Kelly Hu and Sofia Howard-Jimenez
    <br />
    <a href="https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html"
      >https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html</a
    >

    -- head to the link for working GIFs! :]
    <br /><br />
    <hr />
    <br />
    <h2>Overview</h2>
    <p>
      In Project 3-1, we successfully implemented a pathtracer rendering
      program, including its most fundamental functionalities. Path-tracing is
      defined as a rendering technique which produces realistic, photo-like
      images using global illumination, simulated through interpreting the
      behaviour of light.
    </p>
    <h2>Part 1: Ray Generation & Primitive Intersection</h2>
    <h3>
      Walk through the ray generation and primitive intersection parts of the
      rendering pipeline.
    </h3>
    <p>
      Ray generation and primitive intersection are key parts of the rendering
      pipeline. They are responsible for generating rays that mimic the
      behaviour of light in a given scene. They do so by calculating the
      interaction between rays and objects in the scene. We start generating
      rays for each pixel of the image using the camera. Each pixel has a ray
      generated with the origin being the camera's position, and towards the
      direction of the pixel's image plane location. We do this by transforming
      the coordinates of the pixel from the image to the camera space, then
      normalizing the final vector. The technique is referred to as a ray
      tracing method that can apply lighting to an object.
    </p>
    <h3>
      Explain the triangle intersection algorithm you implemented in your own
      words.
    </h3>
    <p>
      The triangle intersection algorithm we implemented involves leveraging the
      Moller Trumbore algorithm learned in class. This algorithm takes the 3
      coordinates of the triangle and also its barycentric coordinates, and
      tests if the ray hits any space within the triangle or not. The ray-scene
      intersection is important as it will permit us to calculate and visualize
      the shadows, lighting, and overall visibility of objects from the POV of
      the camera. Once we do the intersection tests, we need to test for
      validity. If the time in which the intersection occurs is somewhere in the
      range of the <code>max_t</code> and <code>min_t</code> values, it will be
      valid, and an intersection exists. Otherwise, the intersection is invalid.
    </p>
    <p>
      We also need to account for spherical primitives, and must implement
      sphere intersection. For spheres, we use similar methodology as above for
      triangles, but instead utilize the origin of the circle along with the
      circle's radius to compute the time of intersection.
    </p>
    <p>
      After implementing the functions in part 1, we are able to render scenes
      of a few small .dae vectors as shown below.
    </p>

    <br />
    <h3>Show images with normal shading for a few small .dae files:</h3>
    <br />
    <div id="row">
      <div>
        <img src="img/CBspheres.png" width="98%" id="figure" />
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </div>
      <div>
        <img src="img/cow.png" width="98%" id="figure" />
        <figcaption>cow.dae</figcaption>
      </div>
    </div>
    <br />
    <div id="row">
      <div>
        <img src="img/teapot.png" width="98%" id="figure" />
        <figcaption>teapot.dae</figcaption>
      </div>
      <div>
        <img src="img/CBcoil.png" width="98%" id="figure" />
        <figcaption>CBcoil.dae</figcaption>
      </div>
    </div>
    <br />
    <br />

    <h2>Part 2: BVH Construction</h2>
    <h3>
      Walk through your BVH construction algorithm. Explain the heuristic you
      chose for picking the splitting point.
    </h3>
    <p>
      Though we implemented the ray generation and insection methods above,
      rendering very complicated images with various geometric primitives takes
      an extremely long time. With the implementations done in part 1, we need
      to look at each pixel, each ray, and do a scan through all the primitives
      for any possible intersection, even for ones that don't intersect the
      objects. This simply would not scale well. BVH is leveraged to help
      accelerate this process, through reducing the number of intersection tests
      needed between ray and scene objects. BVH uses a bounding volume hierarchy
      which is a binary tree of geometric primitives; the primitives are
      essentially encapsulated by bounding volumes or boxes that represent the
      tree's leaf nodes.
    </p>
    <p>
      Our <code>construct_bvh</code> function starts by initializing a bounding
      box that encloses all the primitives in the scene. It then computes the
      centroid of each primitive and adds it to a running sum of centroids. The
      function also keeps track of the number of primitives in the scene.
    </p>
    <p>
      If the number of primitives is less than or equal to the maximum leaf size
      <code>max_leaf_size</code>, the function creates a leaf node that contains
      all the primitives and returns it. Otherwise, we computes "best" axis
      <code>bestAxis</code>
      by finding one that spans the longest of the bounding box. Then, we
      computes the midpoint of the centroids of all the primitives along that
      axis. The primitives are then partitioned into two groups based on their
      centroids' positions relative to the midpoint found.
    </p>
    <p>
      The function then recursively calls itself on the left and right
      partitions made to construct two child nodes. The child nodes are
      connected to the current node, and the current node is returned. This
      recursion works as, the <code>construct_bvh</code> function builds a BVH
      by recursively splitting the scene into smaller and smaller groups until
      each group contains at most the maximum leaf size number of primitives,
      which is given.
    </p>
    <div>
      <img src="img/cowsplit.gif" width="98%" id="figure" />
      <figcaption>
        GIF of our box splitting (we found this very cool)
      </figcaption>
    </div>
    <br />
    <h3>
      Show images with normal shading for a few large .dae files that you can
      only render with BVH acceleration:
    </h3>
    <br />
    <div id="row">
      <div>
        <img src="img/CBlucy.png" width="98%" id="figure" />
        <figcaption>CBlucy.dae</figcaption>
      </div>
      <div>
        <img src="img/maxplanck.png" width="98%" id="figure" />
        <figcaption>maxplanck.dae</figcaption>
      </div>
    </div>
    <br />
    <h3>
      Compare rendering times on a few scenes with moderately complex geometries
      with and without BVH acceleration. Present your results in a one-paragraph
      analysis.
    </h3>
    <p>
      For more simple renderings like <code>CBspheres_lambertian.dae</code>, we
      saw that the BVH implementation didn't really have much of an impact on
      rendering time, since there are only 14 primitives in that file and thus a
      linear scan would suffice. When we tested other renderings however,
      particularly those with very complex structures and many more primitives,
      the BVH algorithm drastically decreased their rendering times by
      minimizing the number of tests needed to execute and leveraging binary
      lookups. Some renderings that became much faster included
      <code>cow.dae</code> and <code>CBlucy.dae</code> One tradeoff is that
      there is a lot more pre-processing needed for constructing the BVH, but
      this becomes minute when we compare the time differences between non-BVH
      and BVH code.
    </p>
    <br /><br />

    <h2>Part 3</h2>
    <h3>Walk through both implementations of the direct lighting function.</h3>
    <p>
      In Part 3, we implement direct lighting, which kickstarts our process of
      modelling shading that is more realistic. At a high level, we estimate
      direct illumination by projecting/casting shadow ray(s) from each ray to
      surface intersection point. Illumination only occurs if and only if a
      shadow ray intersects with a light source.
    </p>
    <p>
      We start by implementing the diffuse BSDF and zero bounce illumination.
      Both of these functions are responsible for estimating the contribution of
      direct lighting at a given intersection point. The difference between the
      two is that the first function, estimate_direct_lighting_hemisphere, uses
      uniform sampling in a hemisphere to estimate direct lighting, while the
      second function, estimate_direct_lighting_importance, uses importance
      sampling.
    </p>
    <p>
      The estimate_direct_lighting_hemisphere function first constructs a
      coordinate system with the hit point normal aligned with the Z-axis (0, 0,
      1). It then generates a random direction in the hemisphere above the hit
      point using a hemisphere sampler. For each sample, it transforms the
      sample into world space, generates a shadow ray, and checks if the ray
      intersects any object in the scene. If it does not intersect any objects,
      it calculates the contribution of the light by multiplying the light's
      emission, the BSDF of the surface at the intersection point, and the
      cosine of the angle between the sampled direction and the surface normal.
      Finally, it averages the contributions over all the samples and returns
      the result.
    </p>
    <p>
      The estimate_direct_lighting_importance function also constructs a
      coordinate system with the hit point normal aligned with the Z-axis. It
      then loops over all the lights in the scene and for each light, it
      generates a random direction using importance sampling according to the
      probability density function (pdf) of the light. It generates a shadow
      ray, checks for intersection with objects, and calculates the light's
      contribution using the same method as in the
      estimate_direct_lighting_hemisphere function. Finally, it averages the
      contributions over all the lights and samples and returns the result.
    </p>
    <br />
    <h3>
      Show some images rendered with both implementations of the direct lighting
      function. Focus on one particular scene with at least one area light and
      compare the noise levels in soft shadows when rendering with 1, 4, 16, and
      64 light rays (the -l flag) and with 1 sample per pixel (the -s flag)
      using light sampling, not uniform hemisphere sampling.
    </h3>
    <br />
    <div id="row">
      <div>
        <img src="img/bunny_1_1.png" width="98%" id="figure" />
        <figcaption>1 light ray</figcaption>
      </div>
      <div>
        <img src="img/bunny_4_1.png" width="98%" id="figure" />
        <figcaption>4 light rays</figcaption>
      </div>
    </div>
    <br />
    <br />
    <div id="row">
      <div>
        <img src="img/bunny_16_1.png" width="98%" id="figure" />
        <figcaption>16 light rays</figcaption>
      </div>
      <div>
        <img src="img/bunny_64_1.png" width="98%" id="figure" />
        <figcaption>64 light rays</figcaption>
      </div>
    </div>
    <p>
      Looking at the images rendered above, it can be seen that there is a
      drastic amount of noise that gets reduced as we add more light rays. With
      1 ray, the shadowed area of the bunny is a lot noisier, and reaches a
      pretty large distance into the box. But as we increse rays, the shadow
      begins to get softer and concentrate more around the bunny than into the
      box. The increased number of rays evidently generates more accuracy in the
      estimation of reflected light (less variance). The shadows in the corners
      of the wall also soften as the number of rays increase.
    </p>
    <h3>
      Compare the results between uniform hemisphere sampling and lighting
      sampling in a one-paragraph analysis.
    </h3>
    <p>Lorem ipsum</p>
    <br />
    <br />

    <h2>Part 4</h2>
    <h3>Walk through your implementation of the indirect lighting function.</h3>
    <p>
      In order to implement the indirect lighting function, we had to sum up the
      possible paths that bounces of light could take after the zeroeth or first
      bounces of light. This
    </p>
    <h3>
      Show some images rendered with global (direct and indirect) illumination.
      Use 1024 samples per pixel.
    </h3>
    <div id="row">
      <div>
        <img src="img/before-part4.png" width="98%" id="figure" />
        <figcaption>Teapot before edge flipping</figcaption>
      </div>
      <div>
        <img src="img/after-part4.png" width="98%" id="figure" />
        <figcaption>Teapot after flipping a bunch of edges</figcaption>
      </div>
    </div>
    <h3>
      Pick one scene and compare rendered views first with only direct
      illumination, then only indirect illumination. Use 1024 samples per pixel.
      (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in
      your code to generate these views.)
    </h3>
    <p>Lorem ipsum</p>
    <br />

    <h3>
      For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2,
      3, and 100 (the -m flag). Use 1024 samples per pixel.
    </h3>
    <p>Lorem ipsum</p>
    <br />

    <h3>
      Pick one scene and compare rendered views with various sample-per-pixel
      rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
    </h3>
    <p>Lorem ipsum</p>
    <br />
    <br />

    <h2>Part 5</h2>
    <h3>
      Explain adaptive sampling. Walk through your implementation of the
      adaptive sampling.
    </h3>
    <p>
      After implementing the edge flip, we then implemented the edge split
      operation. The first step in the implementation process for this operation
      was drawing a diagram with every edge, vertex, face, and half edge labled
      before and after the edge split operation. Similarly to edge flip, it was
      then a matter of updating pointers and creating new mesh elements if
      needed. We created the diagrams (shown below) to help us keep track of
      pointers and new mesh elements added. This was super critical to our
      process as it helped us debug and ensure we were updating the correct
      pointers.
    </p>
    <h3>
      Pick two scenes and render them with at least 2048 samples per pixel. Show
      a good sampling rate image with clearly visible differences in sampling
      rate over various regions and pixels. Include both your sample rate image,
      which shows your how your adaptive sampling changes depending on which
      part of the image you are rendering, and your noise-free rendered result.
      Use 1 sample per light and at least 5 for max ray depth.
    </h3>
    <div id="row">
      <div>
        <img src="img/Before.png" width="98%" id="figure" />
        <figcaption>Triangles before splitting</figcaption>
      </div>
      <div>
        <img src="img/After.png" width="98%" id="figure" />
        <figcaption>Triangles after splitting</figcaption>
      </div>
    </div>

    <br />
    <br />
    <p>
      This problem was also very tedious to debug! We checked our diagrams and
      code many many times and ensured our pointers were updated correctly.
      However, just like task 4, we had to redo the implementation a few times
      before getting everything right. Debugging part 5 was especially tedious
      due to it having many more elements to handle, and new mesh elements to
      take into consideration. This made it extra confusing to keep track of,
      and we ran into segfaulting issues after typing out our intitial attempt.
      We tried cross-checking our diagram and implementation but couldn't find
      the error.
    </p>

    <br /><br />
    <hr />
    <br />
    <p>Link to Website:</p>
    <a href="https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html"
      >https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html</a
    >
  </body>
</html>
