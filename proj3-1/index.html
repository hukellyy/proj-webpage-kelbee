<html>
  <head>
    <link rel="stylesheet" href="./style.css" />
  </head>

  <body>
    <h1>Project 3-1: Pathtracer ðŸ•º</h1>
    by Kelly Hu and Sofia Howard-Jimenez
    <br />
    <a href="https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html"
      >https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html</a
    >

    -- head to the link for working GIFs! :]
    <br /><br />
    <hr />
    <br />
    <h2>Overview</h2>
    <p>
      In Project 3-1, we successfully implemented a pathtracer rendering
      program, including its most fundamental functionalities. Path-tracing is
      defined as a rendering technique which produces realistic, photo-like
      images using global illumination, simulated through interpreting the
      behaviour of light.
    </p>
    <h2>Part 1: Ray Generation & Primitive Intersection</h2>
    <h3>
      Walk through the ray generation and primitive intersection parts of the
      rendering pipeline.
    </h3>
    <p>
      Ray generation and primitive intersection are key parts of the rendering
      pipeline. They are responsible for generating rays that mimic the
      behaviour of light in a given scene. They do so by calculating the
      interaction between rays and objects in the scene. We start generating
      rays for each pixel of the image using the camera. Each pixel has a ray
      generated with the origin being the camera's position, and towards the
      direction of the pixel's image plane location. We do this by transforming
      the coordinates of the pixel from the image to the camera space, then
      normalizing the final vector. The technique is referred to as a ray
      tracing method that can apply lighting to an object.
    </p>
    <h3>
      Explain the triangle intersection algorithm you implemented in your own
      words.
    </h3>
    <p>
      The triangle intersection algorithm we implemented involves leveraging the
      Moller Trumbore algorithm learned in class. This algorithm takes the 3
      coordinates of the triangle and also its barycentric coordinates, and
      tests if the ray hits any space within the triangle or not. The ray-scene
      intersection is important as it will permit us to calculate and visualize
      the shadows, lighting, and overall visibility of objects from the POV of
      the camera. Once we do the intersection tests, we need to test for
      validity. If the time in which the intersection occurs is somewhere in the
      range of the <code>max_t</code> and <code>min_t</code> values, it will be
      valid, and an intersection exists. Otherwise, the intersection is invalid.
    </p>
    <p>
      We also need to account for spherical primitives, and must implement
      sphere intersection. For spheres, we use similar methodology as above for
      triangles, but instead utilize the origin of the circle along with the
      circle's radius to compute the time of intersection.
    </p>
    <p>
      After implementing the functions in part 1, we are able to render scenes
      of a few small .dae vectors as shown below.
    </p>

    <br />
    <h3>Show images with normal shading for a few small .dae files:</h3>
    <br />
    <div id="row">
      <div>
        <img src="img/CBspheres.png" width="98%" id="figure" />
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </div>
      <div>
        <img src="img/cow.png" width="98%" id="figure" />
        <figcaption>cow.dae</figcaption>
      </div>
    </div>
    <br />
    <div id="row">
      <div>
        <img src="img/teapot.png" width="98%" id="figure" />
        <figcaption>teapot.dae</figcaption>
      </div>
      <div>
        <img src="img/CBcoil.png" width="98%" id="figure" />
        <figcaption>CBcoil.dae</figcaption>
      </div>
    </div>
    <br />
    <br />

    <h2>Part 2: BVH Construction</h2>
    <h3>
      Walk through your BVH construction algorithm. Explain the heuristic you
      chose for picking the splitting point.
    </h3>
    <p>
      Though we implemented the ray generation and insection methods above,
      rendering very complicated images with various geometric primitives takes
      an extremely long time. With the implementations done in part 1, we need
      to look at each pixel, each ray, and do a scan through all the primitives
      for any possible intersection, even for ones that don't intersect the
      objects. This simply would not scale well. BVH is leveraged to help
      accelerate this process, through reducing the number of intersection tests
      needed between ray and scene objects. BVH uses a bounding volume hierarchy
      which is a binary tree of geometric primitives; the primitives are
      essentially encapsulated by bounding volumes or boxes that represent the
      tree's leaf nodes.
    </p>
    <p>
      Our <code>construct_bvh</code> function starts by initializing a bounding
      box that encloses all the primitives in the scene. It then computes the
      centroid of each primitive and adds it to a running sum of centroids. The
      function also keeps track of the number of primitives in the scene.
    </p>
    <p>
      If the number of primitives is less than or equal to the maximum leaf size
      <code>max_leaf_size</code>, the function creates a leaf node that contains
      all the primitives and returns it. Otherwise, we computes "best" axis
      <code>bestAxis</code>
      by finding one that spans the longest of the bounding box. Then, we
      computes the midpoint of the centroids of all the primitives along that
      axis. The primitives are then partitioned into two groups based on their
      centroids' positions relative to the midpoint found.
    </p>
    <p>
      The function then recursively calls itself on the left and right
      partitions made to construct two child nodes. The child nodes are
      connected to the current node, and the current node is returned. This
      recursion works as, the <code>construct_bvh</code> function builds a BVH
      by recursively splitting the scene into smaller and smaller groups until
      each group contains at most the maximum leaf size number of primitives,
      which is given.
    </p>
    <div>
      <img src="img/cowsplit.gif" width="98%" id="figure" />
      <figcaption>
        GIF of our box splitting (we found this very cool)
      </figcaption>
    </div>
    <br />
    <h3>
      Show images with normal shading for a few large .dae files that you can
      only render with BVH acceleration:
    </h3>
    <br />
    <div id="row">
      <div>
        <img src="img/CBlucy.png" width="98%" id="figure" />
        <figcaption>CBlucy.dae</figcaption>
      </div>
      <div>
        <img src="img/maxplanck.png" width="98%" id="figure" />
        <figcaption>maxplanck.dae</figcaption>
      </div>
    </div>
    <br />
    <h3>
      Compare rendering times on a few scenes with moderately complex geometries
      with and without BVH acceleration. Present your results in a one-paragraph
      analysis.
    </h3>
    <p>
      For more simple renderings like <code>CBspheres_lambertian.dae</code>, we
      saw that the BVH implementation didn't really have much of an impact on
      rendering time, since there are only 14 primitives in that file and thus a
      linear scan would suffice. When we tested other renderings however,
      particularly those with very complex structures and many more primitives,
      the BVH algorithm drastically decreased their rendering times by
      minimizing the number of tests needed to execute and leveraging binary
      lookups. Some renderings that became much faster included
      <code>cow.dae</code> and <code>CBlucy.dae</code> One tradeoff is that
      there is a lot more pre-processing needed for constructing the BVH, but
      this becomes minute when we compare the time differences between non-BVH
      and BVH code.
    </p>
    <br /><br />

    <h2>Part 3</h2>
    <h3>Walk through both implementations of the direct lighting function.</h3>
    <p>
      In Part 3, we implement direct lighting, which kickstarts our process of
      modelling shading that is more realistic. At a high level, we estimate
      direct illumination by projecting rays from the surface intersection
      point. Illumination only occurs if and only if the ray intersects with a
      light source.
    </p>
    <p>
      We start by implementing the diffuse BSDF and zero bounce illumination.
      Both of these functions are responsible for estimating the contribution of
      direct lighting at a given intersection point. The difference between the
      two is that the first function, estimate_direct_lighting_hemisphere, uses
      uniform sampling in a hemisphere around the intersection point to estimate
      direct lighting, while the second function,
      estimate_direct_lighting_importance, uses importance sampling.
    </p>
    <p>
      The estimate_direct_lighting_hemisphere function first constructs a
      coordinate system with the hit point normal aligned with the Z-axis (0, 0,
      1). It then generates a random direction in the hemisphere above the hit
      point using a hemisphere sampler. For each sample, it transforms the
      sample into world space, generates a ray, and checks if the ray intersects
      any object in the scene. If it does not intersect any objects, it
      calculates the contribution of the light by multiplying the light's
      emission, the BSDF of the surface at the intersection point, and the
      cosine of the angle between the sampled direction and the surface normal.
      Finally, it averages the contributions over all the samples and returns
      the result.
    </p>
    <p>
      The estimate_direct_lighting_importance function also constructs a
      coordinate system with the hit point normal aligned with the Z-axis. It
      then loops over all the lights in the scene and for each light, it
      generates a random direction using importance sampling according to the
      probability density function (pdf) of the light. It generates a ray,
      checks for intersection with objects, and calculates the light's
      contribution using the same method as in the
      estimate_direct_lighting_hemisphere function. Finally, it averages the
      contributions over all the lights and samples and returns the result.
    </p>
    <br />
    <h3>
      Show some images rendered with both implementations of the direct lighting
      function. Focus on one particular scene with at least one area light and
      compare the noise levels in soft shadows when rendering with 1, 4, 16, and
      64 light rays (the -l flag) and with 1 sample per pixel (the -s flag)
      using light sampling, not uniform hemisphere sampling.
    </h3>
    <br />
    <div id="row">
      <div>
        <img src="img/bunny_1_1.png" width="98%" id="figure" />
        <figcaption>1 light ray</figcaption>
      </div>
      <div>
        <img src="img/bunny_4_1.png" width="98%" id="figure" />
        <figcaption>4 light rays</figcaption>
      </div>
    </div>
    <br />
    <br />
    <div id="row">
      <div>
        <img src="img/bunny_16_1.png" width="98%" id="figure" />
        <figcaption>16 light rays</figcaption>
      </div>
      <div>
        <img src="img/bunny_64_1.png" width="98%" id="figure" />
        <figcaption>64 light rays</figcaption>
      </div>
    </div>
    <p>
      Looking at the images rendered above, it can be seen that there is a
      drastic amount of noise that gets reduced as we add more light rays. With
      1 ray, the shadowed area of the bunny is a lot noisier, and reaches a
      pretty large distance into the box. But as we increase rays, the shadow
      begins to get softer and concentrate more around the bunny than into the
      box. The increased number of rays evidently generates more accuracy in the
      estimation of reflected light (less variance). The shadows in the corners
      of the wall also soften as the number of rays increase.
    </p>
    <h3>
      Compare the results between uniform hemisphere sampling and lighting
      sampling in a one-paragraph analysis.
    </h3>
    <p>
      The results from uniform sampling are much noiser in general than the
      results from importance sampling the lights because they sample uniformly
      around the intersection point rather than from the light sources in the
      scene themselves. The results from uniform sampling also are unable to
      render images that only have point lights, whereas the lighting sampling
      algorithm saves time on point lights by only sampling once. The results
      for uniform hemisphere sampling and lighting sampling do converge to the
      same image as the number of samples increases since they are built from
      the same mesh, but for lower sample-per-pixel rates the lighting sampling
      greatly out performs the uniform hemisphere sampling because the uniform
      sampling leaves black dots throughut the image since each pixel is
      sampling uniformly, whereas with lighting sampling each pixel is sampling
      from the same set of lights, so noise is more concentrated in areas of
      shadow.
    </p>
    <br />
    <br />

    <h2>Part 4</h2>
    <h3>Walk through your implementation of the indirect lighting function.</h3>
    <p>
      In order to implement the indirect lighting function, we had to sum up the
      possible paths that bounces of light could take after the zeroeth or first
      bounces of light. First, we had to implement <code>sample_f</code>, which
      represents a diffuse material that reflects incoming light equally across
      all directions on the hemisphere. This function samples the incoming ray,
      and we implemented it by calling <code>get_sample</code> on the
      <code>sampler</code> variable, which samples an incoming direction and
      writes that into <code>wi</code>.
    </p>
    <p>
      Once we implemented sample_f, we now are able to implement
      <code>at_least_one_bounce_radiance</code>, which recursively calls
      <code>one_bounce_radiance</code> for each bounce of light and stops using
      a Russian Roulette algorithm that terminates with probability 0.4. Within
      <code>one_bounce_radiance</code>, if the algorithm is not terminated by
      Russian Roulette and if the depth of the ray is less than or equal to 1,
      then we will call <code>sample_f</code> based on the BSDF at the current
      <code>Intersection</code>, trace a ray in that direction, then recursively
      call <code>at_least_one_bounce_radiance</code>
      by adding the recursive output to the current output.
    </p>
    <p>
      After <code>at_least_one_bounce_radiance</code> was updated, we also had
      to update <code>est_radiance_global_illumination</code> so that it returns
      <code
        >L_out = zero_bounce_radiance(r, isect) +
        at_least_one_bounce_radiance(r, isect);</code
      >, which combines the direct and indirect lighting estimates.
    </p>
    <h3>
      Show some images rendered with global (direct and indirect) illumination.
      Use 1024 samples per pixel.
    </h3>
    <div id="row">
      <div>
        <img src="img/part4_banana.png" width="98%" id="figure" />
      </div>
      <div>
        <img src="img/part4_bunny.png" width="98%" id="figure" />
      </div>
    </div>
    <h3>
      Pick one scene and compare rendered views first with only direct
      illumination, then only indirect illumination. Use 1024 samples per pixel.
      (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in
      your code to generate these views.)
    </h3>
    <div id="row">
      <div>
        <img src="img/direct_light.png" width="98%" id="figure" />
        <figcaption>direct illumination </figcaption>
      </div>
      <div>
        <img src="img/indirect_light.png" width="98%" id="figure" />
        <figcaption>indirect illumination </figcaption>
      </div>
    </div>
    <br />

    <h3>
      For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2,
      3, and 100 (the -m flag). Use 1024 samples per pixel.
    </h3>
    <div id="row">
      <div>
        <img src="img/max_ray_0.png" width="98%" id="figure" />
        <figcaption>max_ray_depth = 0</figcaption>
      </div>
      <div>
        <img src="img/max_ray_1.png" width="98%" id="figure" />
        <figcaption>max_ray_depth = 1</figcaption>
      </div>
    </div>
    <div id="row">
      <div>
        <img src="img/max_ray_2.png" width="98%" id="figure" />
        <figcaption>max_ray_depth = 2</figcaption>
      </div>
      <div>
        <img src="img/max_ray_3.png" width="98%" id="figure" />
        <figcaption>max_ray_depth = 3</figcaption>
      </div>
      <div>
        <img src="img/max_ray_100.png" width="98%" id="figure" />
        <figcaption>max_ray_depth = 100</figcaption>
      </div>
    </div>
    <br />

    <h3>
      Pick one scene and compare rendered views with various sample-per-pixel
      rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
    </h3>
    <div id="row">
      <div>
        <img src="img/sphere_1.png" width="98%" id="figure" />
        <figcaption>sample-per-pixel = 1</figcaption>
      </div>
      <div>
        <img src="img/sphere_2.png" width="98%" id="figure" />
        <figcaption>sample per pixel = 2</figcaption>
      </div>
    </div>
    <div id="row">
      <div>
        <img src="img/sphere_3.png" width="98%" id="figure" />
        <figcaption>sample per pixel = 3</figcaption>
      </div>

      <div>
        <img src="img/sphere_4.png" width="98%" id="figure" />
        <figcaption>sample per pixel = 4</figcaption>
      </div>
    </div>
    <div id="row">
      <div>
        <img src="img/sphere_8.png" width="98%" id="figure" />
        <figcaption>sample per pixel = 8</figcaption>
      </div>
      <div>
        <img src="img/sphere_16.png" width="98%" id="figure" />
        <figcaption>sample per pixel = 16</figcaption>
      </div>
    </div>
    <div id="row">
      <div>
        <img src="img/sphere_64.png" width="98%" id="figure" />
        <figcaption>sample per pixel = 64</figcaption>
      </div>
      <div>
        <img src="img/sphere_1024.png" width="98%" id="figure" />
        <figcaption>sample per pixel = 1024</figcaption>
      </div>
    </div>

    <br />
    <br />

    <h2>Part 5</h2>
    <h3>
      Explain adaptive sampling. Walk through your implementation of the
      adaptive sampling.
    </h3>
    <p>
      In part 5 of the project, we added adaptive sampling to reduce the noise
      in the image, which concentrates the samples in more difficult parts of
      the image as opposed to using a fixed number of samples per pixel.
    </p>
    <p>
      We implemented adaptive sampling by updating
      <code>raytrace_pixel</code> to detect whether the pixel has converged as
      we trace the samples thrrough that pixel. In order to do this, we solve
      for the pixels convergence
      <code>double I = 1.96 * (standard_deviation / sqrt(sample_number))</code>
      and check if <code>I <= (maxTolerance * mean)</code>, for which we would
      say the pixel has converged and we stop tracing rays for this pixel. we
      calculated the standard deviation by solving for the variance using the
      sum of the illuminance and sum of the illuminance squared for the pixel
      and taking the square root of said variance.
    </p>
    <h3>
      Pick two scenes and render them with at least 2048 samples per pixel. Show
      a good sampling rate image with clearly visible differences in sampling
      rate over various regions and pixels. Include both your sample rate image,
      which shows your how your adaptive sampling changes depending on which
      part of the image you are rendering, and your noise-free rendered result.
      Use 1 sample per light and at least 5 for max ray depth.
    </h3>
    <div id="row">
      <div>
        <img src="img/part5_bunny.png" width="98%" id="figure" />
      </div>
      <div>
        <img src="img/part5_sphere.png" width="98%" id="figure" />
      </div>
    </div>
    <div id="row">
      <div>
        <img src="img/part5_bunny_sr.png" width="98%" id="figure" />
        <figcaption>sample rate image for the bunny</figcaption>
      </div>
      <div>
        <img src="img/part5_sphere_sr.png" width="98%" id="figure" />
        <figcaption>sample rate image for the spheres</figcaption>
      </div>
      </div>
    </div>

    <br /><br />
    <hr />
    <br />
    <p>Link to Website:</p>
    <a href="https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html"
      >https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html</a
    >
  </body>
</html>
