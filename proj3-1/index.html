<html>
  <head>
    <link rel="stylesheet" href="./style.css" />
  </head>

  <body>
    <h1>Project 3-1: Pathtracer ðŸ•º</h1>
    by Kelly Hu and Sofia Howard-Jimenez
    <br />
    <a href="https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html"
      >https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html</a
    >

    -- head to the link for working GIFs! :]
    <br /><br />
    <hr />
    <br />
    <h2>Overview</h2>
    <p>
      In Project 3-1, we successfully implemented a pathtracer rendering program, 
	  including its most fundamental functionalities. Path-tracing is defined as 
	  a rendering technique which produces realistic, photo-like images using global 
	  illumination, simulated through interpreting the behaviour of light.
    </p>
    <h2>Part 1: Ray Generation & Primitive Intersection</h2>
	<h3>Walk through the ray generation and primitive intersection parts of the rendering pipeline.
  </h3>
    <p>
      To generate rays, we used the image's coordinate system and transformed it into 
      the camera space. We do this by taking in a point 
    </p>
	<h3>Explain the triangle intersection algorithm you implemented in your own words.  </h3>
    <p>
      First, the algorithm does linear interpolation between pairs of adjacent
      points and lays down a new control point on each edge. Since the algorithm
      is recursive, this step is repeated, and a new control point is placed on
      each new edge until there is only one point left.
    </p>
    <br />
    <h3>Show images with normal shading for a few small .dae files: </h3>
    <br />
    <div id="row">
      <div>
        <img src="img/CBspheres.png" width="98%" id="figure" />
        <figcaption>Rendering of CBspheres_lambertian.dae</figcaption>
      </div>
      <div>
        <img src="img/cow.png" width="98%" id="figure" />
        <figcaption>
          A slightly different curve & modifying parameter t
        </figcaption>
      </div>
    </div>
    <br />
    <div id="row">
      <div>
        <img src="img/teapot.png" width="98%" id="figure" />
        <figcaption>Rendering of CBspheres_lambertian.dae</figcaption>
      </div>
      <div>
        <img src="img/CBcoil.png" width="98%" id="figure" />
        <figcaption>
          A slightly different curve & modifying parameter t
        </figcaption>
      </div>
    </div>
    <br />
    <br />

    <h2>Part 2</h2>
    <h3>Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
    </h3>
    <p>
      Building off the concept of Bezier curves, we will now extend the idea to
      add another dimension. Bezier surfaces are also defined by a set of
      control points, where a Bezier surface patch consists of 4x4 control
      points. To visualize or implement this, we can imagine having 4 curves in
      the u-direction, or 4x1 control points in the u-direction. Then, we can
      define 4 more control points in the v-direction as "moving curves", which
      are essentially sweeping out the 2D surface / connecting the 4 curves made
      in the u direction.
    </p>
    <br />
    <h3>Show images with normal shading for a few large .dae files that you can only render with BVH acceleration: </h3>
    <br />
    <div id="row">
      <div>
        <img src="img/CBlucy.png" width="98%" id="figure" />
        <figcaption>bez/teapot.bez</figcaption>
      </div>
      <div>
        <img src="img/maxplanck.png" width="98%" id="figure" />
        <figcaption>bez/teapot.bez with wireframe off</figcaption>
      </div>
    </div>
    <br />
    <h3>Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
    </h3>
    <p>
      Lorem ipsum
    </p>
    <br /><br />

    <h2>Part 3</h2>
    <h3>Walk through both implementations of the direct lighting function.</h3>
    <p>
      In Part 3, we were tasked with implementing area-weighted vertex normals
      to use for Phong shading. This allows us to better shade smooth surfaces
      as it produces the "smoothing" effect by performing linear interpolations
      on the vertex normals as opposed to face normals.
    </p>
    <br />
    <h3>Show some images rendered with both implementations of the direct lighting function:</h3>
    <br />
    <div id="row">
      <div>
        <img src="img/part3-nophong.png" width="98%" id="figure" />
        <figcaption>bez/teapot.bez without smooth shading</figcaption>
      </div>
      <div>
        <img src="img/part3-phong.png" width="98%" id="figure" />
        <figcaption>bez/teapot.bez with smooth shading</figcaption>
      </div>
    </div>
    <br />
    <br />
    <div id="row">
      <div>
        <img src="img/part3-close.png" width="98%" id="figure" />
        <figcaption>bez/teapot.bez without smooth shading</figcaption>
      </div>
      <div>
        <img src="img/part3-phong-close.png" width="98%" id="figure" />
        <figcaption>bez/teapot.bez with smooth shading</figcaption>
      </div>
    </div>
    <h3>Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
    </h3>
    <p>
      In order to compute an area-weighted normal for a given vertex, we first
      iterated through the faces incident to the vertex and found the face
      normal multiplied by the face's area for each face. We found the area from
      the Vertex positions for that face. We then added the weighted face normal
      to the norm sum (which we initilized to be 0). After iterating through all
      the faces, we then averaged the norm sum by dividing by the number of
      faces and returned the normalized norm sum.
    </p>
    <h3>Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.</h3>
    <p>Lorem ipsum

    </p>
    <br />
    <br />

    <h2>Part 4</h2>
    <h3>Walk through your implementation of the indirect lighting function. </h3>
    <p>
      To implement the edge flip operation, which essentially performs the
      function of flipping an edge, we started by outlining our approach as
      needing to reassign a bunch of pointers for halfedges, vertices, edges,
      and faces, to a new set of correct halfedges, vertices, edges, and faces.
      We then proceeded to jot down and initialize all the pointers necessary,
      using a diagram to keep track of all the halfedges, faces, vertices,
      face's edges, and corresponding halfedges that we need to update,
      illustrating what it should look like before and after the flip. Then, we
      started to reassign everything carefully, according to the diagrams drawn.
      This was definitely a very tedious process, and we had to go back and redo
      some reassignments due to typos or wrong pointers, but eventually we were
      able to reassign everything mostly correctly. To debug, we ran through the
      code multiple times to figure out if we missed a pointer or assigned one
      incorrectly by mistake. After a few iterations of this, we ended up with a
      working edge flip! Finally, we also identified an edge case where we
      checked if the edge is a boundary, in which we return right away.
      Thankfully, we didn't need to endure too much debugging for this part, but
      Part 5 was definitely an interesting experience...
    </p>
    <h3>Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
    </h3>
    <div id="row">
      <div>
        <img src="img/before-part4.png" width="98%" id="figure" />
        <figcaption>Teapot before edge flipping</figcaption>
      </div>
      <div>
        <img src="img/after-part4.png" width="98%" id="figure" />
        <figcaption>Teapot after flipping a bunch of edges</figcaption>
      </div>
    </div>
    <h3>Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
    </h3>
    <p>Lorem ipsum</p>
    <br />

    <h3>For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
    </h3>
    <p>Lorem ipsum</p>
    <br />

    <h3>Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
    </h3>
    <p>Lorem ipsum</p>
    <br />
    <br />

    <h2>Part 5</h2>
    <h3>Explain adaptive sampling. Walk through your implementation of the adaptive sampling.    </h3>
    <p>
      After implementing the edge flip, we then implemented the edge split
      operation. The first step in the implementation process for this operation
      was drawing a diagram with every edge, vertex, face, and half edge labled
      before and after the edge split operation. Similarly to edge flip, it was
      then a matter of updating pointers and creating new mesh elements if
      needed. We created the diagrams (shown below) to help us keep track of
      pointers and new mesh elements added. This was super critical to our
      process as it helped us debug and ensure we were updating the correct
      pointers.
    </p>
    <h3>Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
    </h3>
    <div id="row">
      <div>
        <img src="img/Before.png" width="98%" id="figure" />
        <figcaption>Triangles before splitting</figcaption>
      </div>
      <div>
        <img src="img/After.png" width="98%" id="figure" />
        <figcaption>Triangles after splitting</figcaption>
      </div>
    </div>

    <br />
    <br />
    <p>
      This problem was also very tedious to debug! We checked our diagrams and
      code many many times and ensured our pointers were updated correctly.
      However, just like task 4, we had to redo the implementation a few times
      before getting everything right. Debugging part 5 was especially tedious
      due to it having many more elements to handle, and new mesh elements to
      take into consideration. This made it extra confusing to keep track of,
      and we ran into segfaulting issues after typing out our intitial attempt.
      We tried cross-checking our diagram and implementation but couldn't find
      the error.
    </p>
  
    <br /><br />
    <hr />
    <br />
    <p>Link to Website:</p>
    <a href="https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html"
      >https://hukellyy.github.io/proj-webpage-kelbee/proj3-1/index.html</a
    >
  </body>
</html>
